Created new wandb run! emrrriwq
Learner successfully initialized!
Press (p) to pause (c) to checkpoint, (q) to checkpoint and quit (after next iteration)

--------BEGIN ITERATION REPORT--------
Policy Reward: 0.00044
Policy Entropy: 4.49871
Value Function Loss: nan

Mean KL Divergence: 0.00000
SB3 Clip Fraction: 0.00000
Policy Update Magnitude: 0.02684
Value Function Update Magnitude: 0.02807

Collected Steps per Second: 20,824.38319
Overall Steps per Second: 13,406.98595

Timestep Collection Time: 2.40103
Timestep Consumption Time: 1.32837
PPO Batch Consumption Time: 0.73528
Total Iteration Time: 3.72940

Cumulative Model Updates: 1
Cumulative Timesteps: 50,000

Timesteps Collected: 50,000
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: -0.13591
Policy Entropy: 4.49873
Value Function Loss: 0.03707

Mean KL Divergence: 0.00000
SB3 Clip Fraction: 0.00000
Policy Update Magnitude: 0.02189
Value Function Update Magnitude: 0.01828

Collected Steps per Second: 22,278.12646
Overall Steps per Second: 14,202.45906

Timestep Collection Time: 2.24435
Timestep Consumption Time: 1.27616
PPO Batch Consumption Time: 0.64322
Total Iteration Time: 3.52052

Cumulative Model Updates: 2
Cumulative Timesteps: 100,000

Timesteps Collected: 50,000
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 0.02180
Policy Entropy: 4.49875
Value Function Loss: 0.07134

Mean KL Divergence: 0.00000
SB3 Clip Fraction: 0.00000
Policy Update Magnitude: 0.03392
Value Function Update Magnitude: 0.03611

Collected Steps per Second: 21,646.90989
Overall Steps per Second: 11,690.13055

Timestep Collection Time: 2.30980
Timestep Consumption Time: 1.96731
PPO Batch Consumption Time: 0.68892
Total Iteration Time: 4.27711

Cumulative Model Updates: 4
Cumulative Timesteps: 150,000

Timesteps Collected: 50,000
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: -0.15097
Policy Entropy: 4.49876
Value Function Loss: 0.11538

Mean KL Divergence: 0.00000
SB3 Clip Fraction: 0.00000
Policy Update Magnitude: 0.04592
Value Function Update Magnitude: 0.05748

Collected Steps per Second: 20,772.93099
Overall Steps per Second: 9,624.06636

Timestep Collection Time: 2.40736
Timestep Consumption Time: 2.78878
PPO Batch Consumption Time: 0.70611
Total Iteration Time: 5.19614

Cumulative Model Updates: 7
Cumulative Timesteps: 200,008

Timesteps Collected: 50,008
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: -0.12754
Policy Entropy: 4.49876
Value Function Loss: 0.13811

Mean KL Divergence: 0.00001
SB3 Clip Fraction: 0.00000
Policy Update Magnitude: 0.04375
Value Function Update Magnitude: 0.06086

Collected Steps per Second: 21,241.45705
Overall Steps per Second: 9,987.02569

Timestep Collection Time: 2.35408
Timestep Consumption Time: 2.65282
PPO Batch Consumption Time: 0.68470
Total Iteration Time: 5.00690

Cumulative Model Updates: 10
Cumulative Timesteps: 250,012

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 0.00231
Policy Entropy: 4.49876
Value Function Loss: 0.11483

Mean KL Divergence: 0.00001
SB3 Clip Fraction: 0.00000
Policy Update Magnitude: 0.04173
Value Function Update Magnitude: 0.06091

Collected Steps per Second: 21,654.43148
Overall Steps per Second: 10,145.49842

Timestep Collection Time: 2.30900
Timestep Consumption Time: 2.61930
PPO Batch Consumption Time: 0.67477
Total Iteration Time: 4.92829

Cumulative Model Updates: 13
Cumulative Timesteps: 300,012

Timesteps Collected: 50,000
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 0.00351
Policy Entropy: 4.49875
Value Function Loss: 0.06887

Mean KL Divergence: 0.00001
SB3 Clip Fraction: 0.00000
Policy Update Magnitude: 0.03849
Value Function Update Magnitude: 0.05797

Collected Steps per Second: 20,370.72421
Overall Steps per Second: 9,687.15104

Timestep Collection Time: 2.45509
Timestep Consumption Time: 2.70762
PPO Batch Consumption Time: 0.66828
Total Iteration Time: 5.16271

Cumulative Model Updates: 16
Cumulative Timesteps: 350,024

Timesteps Collected: 50,012
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 0.00753
Policy Entropy: 4.49874
Value Function Loss: 0.06886

Mean KL Divergence: 0.00001
SB3 Clip Fraction: 0.00000
Policy Update Magnitude: 0.03405
Value Function Update Magnitude: 0.05354

Collected Steps per Second: 21,773.90259
Overall Steps per Second: 10,027.14177

Timestep Collection Time: 2.29688
Timestep Consumption Time: 2.69078
PPO Batch Consumption Time: 0.70050
Total Iteration Time: 4.98766

Cumulative Model Updates: 19
Cumulative Timesteps: 400,036

Timesteps Collected: 50,012
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 0.17506
Policy Entropy: 4.49872
Value Function Loss: 0.09179

Mean KL Divergence: 0.00000
SB3 Clip Fraction: 0.00000
Policy Update Magnitude: 0.03151
Value Function Update Magnitude: 0.05052

Collected Steps per Second: 22,553.44424
Overall Steps per Second: 10,208.48078

Timestep Collection Time: 2.21731
Timestep Consumption Time: 2.68136
PPO Batch Consumption Time: 0.67119
Total Iteration Time: 4.89867

Cumulative Model Updates: 22
Cumulative Timesteps: 450,044

Timesteps Collected: 50,008
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 0.01320
Policy Entropy: 4.49871
Value Function Loss: 0.09147

Mean KL Divergence: 0.00000
SB3 Clip Fraction: 0.00000
Policy Update Magnitude: 0.03304
Value Function Update Magnitude: 0.05126

Collected Steps per Second: 22,224.91524
Overall Steps per Second: 10,210.29008

Timestep Collection Time: 2.25027
Timestep Consumption Time: 2.64793
PPO Batch Consumption Time: 0.68341
Total Iteration Time: 4.89820

Cumulative Model Updates: 25
Cumulative Timesteps: 500,056

Timesteps Collected: 50,012
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 0.27286
Policy Entropy: 4.49870
Value Function Loss: 0.09151

Mean KL Divergence: 0.00000
SB3 Clip Fraction: 0.00000
Policy Update Magnitude: 0.03359
Value Function Update Magnitude: 0.05169

Collected Steps per Second: 22,480.59615
Overall Steps per Second: 10,224.54668

Timestep Collection Time: 2.22467
Timestep Consumption Time: 2.66669
PPO Batch Consumption Time: 0.66972
Total Iteration Time: 4.89137

Cumulative Model Updates: 28
Cumulative Timesteps: 550,068

Timesteps Collected: 50,012
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 0.00961
Policy Entropy: 4.49869
Value Function Loss: 0.06845

Mean KL Divergence: 0.00000
SB3 Clip Fraction: 0.00000
Policy Update Magnitude: 0.03068
Value Function Update Magnitude: 0.05102

Collected Steps per Second: 21,616.08603
Overall Steps per Second: 9,952.89811

Timestep Collection Time: 2.31309
Timestep Consumption Time: 2.71057
PPO Batch Consumption Time: 0.69186
Total Iteration Time: 5.02366

Cumulative Model Updates: 31
Cumulative Timesteps: 600,068

Timesteps Collected: 50,000
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 0.01069
Policy Entropy: 4.49868
Value Function Loss: 0.06811

Mean KL Divergence: 0.00000
SB3 Clip Fraction: 0.00000
Policy Update Magnitude: 0.03086
Value Function Update Magnitude: 0.05163

Collected Steps per Second: 22,011.49881
Overall Steps per Second: 10,041.10810

Timestep Collection Time: 2.27154
Timestep Consumption Time: 2.70799
PPO Batch Consumption Time: 0.66703
Total Iteration Time: 4.97953

Cumulative Model Updates: 34
Cumulative Timesteps: 650,068

Timesteps Collected: 50,000
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: -0.00140
Policy Entropy: 4.49867
Value Function Loss: 0.04545

Mean KL Divergence: 0.00000
SB3 Clip Fraction: 0.00000
Policy Update Magnitude: 0.02846
Value Function Update Magnitude: 0.04782

Collected Steps per Second: 21,060.08699
Overall Steps per Second: 9,953.71705

Timestep Collection Time: 2.37454
Timestep Consumption Time: 2.64951
PPO Batch Consumption Time: 0.67119
Total Iteration Time: 5.02405

Cumulative Model Updates: 37
Cumulative Timesteps: 700,076

Timesteps Collected: 50,008
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 0.23402
Policy Entropy: 4.49865
Value Function Loss: 0.06809

Mean KL Divergence: 0.00000
SB3 Clip Fraction: 0.00000
Policy Update Magnitude: 0.02560
Value Function Update Magnitude: 0.04521

Collected Steps per Second: 21,995.75355
Overall Steps per Second: 10,094.71822

Timestep Collection Time: 2.27371
Timestep Consumption Time: 2.68056
PPO Batch Consumption Time: 0.67473
Total Iteration Time: 4.95427

Cumulative Model Updates: 40
Cumulative Timesteps: 750,088

Timesteps Collected: 50,012
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 0.03448
Policy Entropy: 4.49864
Value Function Loss: 0.09035

Mean KL Divergence: 0.00000
SB3 Clip Fraction: 0.00000
Policy Update Magnitude: 0.02775
Value Function Update Magnitude: 0.04904

Collected Steps per Second: 22,695.11477
Overall Steps per Second: 10,183.89996

Timestep Collection Time: 2.20347
Timestep Consumption Time: 2.70703
PPO Batch Consumption Time: 0.67378
Total Iteration Time: 4.91050

Cumulative Model Updates: 43
Cumulative Timesteps: 800,096

Timesteps Collected: 50,008
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 0.00092
Policy Entropy: 4.49864
Value Function Loss: 0.09027

Mean KL Divergence: 0.00000
SB3 Clip Fraction: 0.00000
Policy Update Magnitude: 0.03040
Value Function Update Magnitude: 0.05234

Collected Steps per Second: 22,663.71318
Overall Steps per Second: 10,266.26561

Timestep Collection Time: 2.20670
Timestep Consumption Time: 2.66479
PPO Batch Consumption Time: 0.68966
Total Iteration Time: 4.87149

Cumulative Model Updates: 46
Cumulative Timesteps: 850,108

Timesteps Collected: 50,012
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 0.03737
Policy Entropy: 4.49863
Value Function Loss: 0.06800

Mean KL Divergence: 0.00000
SB3 Clip Fraction: 0.00000
Policy Update Magnitude: 0.02969
Value Function Update Magnitude: 0.05124

Collected Steps per Second: 21,060.44150
Overall Steps per Second: 9,767.93802

Timestep Collection Time: 2.37431
Timestep Consumption Time: 2.74489
PPO Batch Consumption Time: 0.68657
Total Iteration Time: 5.11920

Cumulative Model Updates: 49
Cumulative Timesteps: 900,112

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 0.02165
Policy Entropy: 4.49862
Value Function Loss: 0.04553

Mean KL Divergence: 0.00000
SB3 Clip Fraction: 0.00000
Policy Update Magnitude: 0.02868
Value Function Update Magnitude: 0.04910

Collected Steps per Second: 21,635.98072
Overall Steps per Second: 9,931.54971

Timestep Collection Time: 2.31152
Timestep Consumption Time: 2.72415
PPO Batch Consumption Time: 0.69807
Total Iteration Time: 5.03567

Cumulative Model Updates: 52
Cumulative Timesteps: 950,124

Timesteps Collected: 50,012
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: -0.09865
Policy Entropy: 4.49861
Value Function Loss: 0.04543

Mean KL Divergence: 0.00000
SB3 Clip Fraction: 0.00000
Policy Update Magnitude: 0.02705
Value Function Update Magnitude: 0.04668

Collected Steps per Second: 20,415.81672
Overall Steps per Second: 9,685.65434

Timestep Collection Time: 2.44947
Timestep Consumption Time: 2.71363
PPO Batch Consumption Time: 0.67884
Total Iteration Time: 5.16310

Cumulative Model Updates: 55
Cumulative Timesteps: 1,000,132

Timesteps Collected: 50,008
--------END ITERATION REPORT--------


Saving checkpoint 1000132...
Checkpoint 1000132 saved!

--------BEGIN ITERATION REPORT--------
Policy Reward: 0.00940
Policy Entropy: 4.49860
Value Function Loss: 0.04545

Mean KL Divergence: 0.00000
SB3 Clip Fraction: 0.00000
Policy Update Magnitude: 0.02624
Value Function Update Magnitude: 0.04462

Collected Steps per Second: 21,974.80525
Overall Steps per Second: 10,174.28457

Timestep Collection Time: 2.27588
Timestep Consumption Time: 2.63965
PPO Batch Consumption Time: 0.67468
Total Iteration Time: 4.91553

Cumulative Model Updates: 58
Cumulative Timesteps: 1,050,144

Timesteps Collected: 50,012
--------END ITERATION REPORT--------
